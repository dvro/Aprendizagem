% !TEX encoding = ISO-8859-1
\chapter{Técnicas Avaliadas}
\label{ch:ch2}

\section{Contexto e Histórico}
\label{sec:contexto}

Esta seção abordará sistemas de reconhecimento de padrões e classificadores com aprendizagem baseadas em instâncias.

\subsection{Sistemas de Reconhecimento de Padrões}
\label{subsec:sistemas}

No final dos anos 50, surgiram os primeiros trabalhos de aprendizagem de máquina. De uma forma geral, elas consistiam em dar ao computador a habilidade de reconhecer formas. A partir daí, surgiram diversos problemas onde a aprendizagem de máquina atuava. 

Existem três problemas gerais que a aprendizagem de máquina tenta resolver. Um deles é o problema do agrupamento, que consiste em agrupar dados de acordo com suas características, de forma que seja possível extrair informação útil destes agrupamentos. Um outro problema é a discriminação, que basicamente é achar uma forma de reconhecer um conceito, dado um conjunto de conceitos exemplos. O terceiro e último problema é o da generalização, que é o problema de como reduzir uma regra de classificação, tornando-a mais abrangente e menos custosa.

Reconhecimento de padrões ataca principalmente o problema da discriminação, tendo por objetivo classificar padrões, discriminando-os entre duas ou mais classes. A classificação pode ser feita com padrões pertencentes a qualquer domínio, como reconhecimento de digitais, gestos, escrita, fala, entre outros.

\section{Agrupamento}

Agrupamento, também conhecido como Clusterização, é a classificação não-supervisionada de dados que forma grupos, ou clusters. Assim, a análise de clusters envolve a organização de um conjunto de padrões em agrupamentos, de acordo com algumas regras de similaridade.

De forma simples, pode-se de dizer que a clusterização é o agrupamento de instâncias baseadas nas suas características, ou seja, instâncias similares pertencem ao mesmo grupo. Estes agrupamentos são formados por padrões não rotulados, que não se sabe a classe de cada padrão, assim, rótulos podem ser definidos após a realização da clusterização.

Uma vantagem da clusterização é que atualmente existem muitos dados, mas poucos destes dados são classificados. Algoritmos de clusterização possibilitam a rotulação de forma mais rápida e factível por um especialista.

\subsection{K-Means}
\label{subsec:teoria-kmeans}

Um dos algoritmos de clusterização mais famosos é o K-means. Este método divide $T$ padrões em $K$ clusters onde cada padrão $x$ pertence ao cluster de média mais próxima de $x$. O algoritmo é repetido até que poucas alterações sejam feitas em cada iteração. O resultado deste algoritmo é uma partição do espaço em Células de Voronoi.

\begin{algorithm}[H]
\caption{K-Means}
\label{alg:kmeans}
\begin{algorithmic}[1]
\REQUIRE {$K$: um número de clusters desejados}
\REQUIRE {$T$: um conjunto de padrões}
\STATE  $M$ = Obtenha $K$ padrões aleatoriamente
\STATE  $G$ = Lista de grupos de instâncias $\in$ ao cluster do representante em $M$
\WHILE {$M$ não convergiu}
\FORALL {$t_i$ $\in$ $T$}
\STATE  $j$ = index de $KNN(t_i, M)$
\STATE  adicione $t_i$ ao grupo $G_j$
\ENDFOR
\STATE atualize $M$ com as médias de $G$
\ENDWHILE
\RETURN os clusters e as médias $G, M$
\end{algorithmic}
\end{algorithm}

\section{Classificadores}
\label{sec:classificadores}

Todo sistema de reconhecimento de padrões utiliza um classificador para discriminar os padrões de teste. A eficiência de um classificador é medida pela taxa de acerto média, pela variância, e pelo seu custo computacional. Um classificador de aprendizagem baseada em instâncias muito utilizado é o \textit{K-Nearest Neighbor}, KNN [\cite{knnrule:1969}]. 

\subsection{K-Nearest Neighbor}
\label{subsec:knn}

O KNN é muito usado por ser um método de aprendizagem supervisionado simples, e por possuir uma taxa de acerto relativamente alta. O conceito básico consiste em: dado um padrão $x$ a ser classificado e um conjunto de padrões conhecidos $T$, obter as classes dos $K$ elementos de $T$ mais próximos de $x$. A classe que obtiver maior ocorrência, ou peso, será a classe de $x$. Pode-se dizer que o KNN utiliza uma abordagem \textit{"Diga-me com quem andas, e direi quem és."}. O algoritmo está descrito em Algorithm \ref{alg:knn}.

\begin{algorithm}[H]
\caption{KNN}
\label{alg:knn}
\begin{algorithmic}[1]
\REQUIRE {$K$: um número}
\REQUIRE {$T$: conjunto de treinamento}
\REQUIRE {$x$: elemento para ser classificado}
\REQUIRE {$L$: uma lista}
\FORALL {$t_i$ $\in$ $T$}
\STATE  $d_i$ = $distance(t_i, x)$
\STATE  adicione $(d_i, Classe(t_i))$ em $L$
\ENDFOR
\STATE $Ordene(L)$ de acordo com as distâncias
\STATE obtenha os $K$ primeiros elementos de $L$
\RETURN a classe de maior ocorrência, ou peso, entre os $K$
\end{algorithmic}
\end{algorithm}


Pode-se dizer que o $K-Nearest Neighbor$ atua considerando a densidade das classes na região do padrão que se deseja classificar. Ou seja, o mesmo calcula diretamente a probabilidade a posteriori.

O \textit{KNN} é uma técnica de estimativa de densidade não paramétrica, onde a densidade é calculada a partir da equação \ref{eq:densidade}, onde $K_n$ é um valor fixo que é função do número de exemplos $n$, $V_n$ é uma variável aleatória que, no caso do \textit{KNN} é a distância da amostra $x$ para o \textit{k-ésimo} vizinho mais próximo.

\begin{equation}
\label{eq:densidade}
p(x) = \frac{\frac{k_n}{n}}{V_n}
\end{equation}

A partir da estimativa de probabilidade a priori (proporção de classes no todo) e estimativa da densidade condicional, encontra-se a probabilidade a posteriori, intuitivamente representada pela regra de decisão do \textit{KNN}:

\begin{equation}
\label{eq:knnposteriori}
P(\omega_i | x) = \dfrac{k_i}{K}
\end{equation}
Onde $k_i$ é a quantidade de instâncias da classe $\omega_i$.


\subsection{Janela de Parzen}
\label{subsec:janeladeparzen}

Janela de Parzen é uma técnica de estimação de densidade não-paramétrica que se baseia na interpolação de dados. O estimador tenta interporlar os dados a partir de usos de janelas, como em um histograma, por exemplo, afim de estimar a densidade a partir da frequência das amostras encontradas em cada janela.

Esta estimativa é feita assumindo-se uma função $K(x)$ que determina a janela centrada em $x$ com largura $h$, esta função é chamada de função de kernel.

De maneira simples, para estimar a densidade fixa-se uma região $R$ e um volume fixo $V$, e $k$ correspondente é determinado a partir dos dados de aprendizagem. Assumindo que a região $R$ é um hipercubo de $d$ dimensões, o seu volume é dado por $h^d$.

Para se obter a função de densidade de probabilidade de uma amostra $x$ utilizando-se a Janela de Parzen, simplesmente centra-se a região $R$ em $x$ e conta-se o número de instâncias dentro desta região (valor de $k$) na equação \ref{eq:densidade}.

Determinar o $h$ ideal é uma tarefa minunciosa, pois, se o valor de $h$ for muito pequeno, haverá muita especialização e estará mais sujeita a erros ocasionados por ruídos, já se $h$ for muito grande, ocorrerá uma super-generalização. A escolha da função $K(x)$ apropriada também é um problema para esta técnica.

\subsection{Estimação da Máxima Verossimilhança}
\label{subsec:mle}

Estimativa por máxima verossimilhança, $Maximum-Likelihood$, é um método para estimar os parâmetros de um modelo estatístico. A partir de um conjunto de dados e um dado modelo estatístico (i.e. Normal), o $MLE$ estima valores adequados para os parâmetros do modelo.

De forma geral, dado um conjunto de instâncias de treinamento e um modelo estatístico, o $MLE$ estima os valores dos parâmetros do modelo estatístico de forma que a probabilidade dos dados observados sejam maximizadas, conforme a equação \ref{eq:mle}.

\begin{equation}
\label{eq:mle}
\hat{\theta} = \operatorname*{arg\,max}_{\theta} \ln P(D | \theta)
\end{equation}

Onde $\hat{\theta}$ é o valor de $\theta$ mais compatível com os exemplos observados, $D$ é o conjunto de exemplo, e $P(D | \theta)$ é a verossimilhança de $\theta$ em relação ao conjunto de exemplos.

Para o caso de uma distribuição normal, o modelo é definido pelo vetor de médias e pela matrix de covariância. Neste caso, pode-se mostrar que as estimativas de máxima verossimilhança para os parâmetros são dados pelas equações \ref{eq:mle_mean_normal} e \ref{eq:mle_sigma_normal}.

\begin{equation}
\label{eq:mle_mean_normal}
\hat{\mu} = \frac{1}{n} \sum_{k = 1}^{n} \mathbf{x_k}
\end{equation}

\begin{equation}
\label{eq:mle_sigma_normal}
\hat{\Sigma } =  \dfrac{1}{n} \sum_{k = 1}^{n} (\mathbf{x_k} - \hat{\mu}) (\mathbf{x_k} - \hat{\mu})^t
\end{equation}

O algoritmo \textit{MLE} só é capaz de tratar distribuições unimodais, para distribuições multimodais, utiliza-se o algoritmo \textit{EM}, abordado a seguir.

\subsection{Algoritmo Expectation-Maximization}
\label{subsec:em}
Expectation-Maximization é um algoritmo iterativo para se encontrar a máxima verossimilhança, os parâmetros de um modelo, onde o modelo é desconhecido e depende de variáveis não obsersevadas. 

A iteração do $EM$ alterna as etapas $E$ (Expectation) e $M$ (Maximization). A etapa $E$ computa o valor esperado da log-verossimilhança avaliada, utilizando a estimativa atual para os parâmetros. Já a etapa $M$ computa os parâmetros, maximizando a esperada log-verossimilhança encontrada na etapa $E$.

A diferença do $EM$ para o $MLE$ é que o $EM$ é capaz de tratar distribuições multimodais, além de ser capaz de estimar os parâmetros mesmo com \textit{missing values}.

\section{Combinação de Classificadores}

Combinar classificadores é a ação de utilizar dois ou mais classificadores para classificar um dado padrão. No caso, a ideia é obter o melhor de cada classificador. A combinação de classificadores deve ser feita utilizando classificadores que diferem, que classificam um mesmo padrão de forma divergente.

Por combinar \textit{"o melhor"} dos classificadores, esta técnica é interessantes para tratar de regiões de indecisão, driblando limitações dos classificadores utilizados, pois cada um fornece uma informação complementar na tarefa de classificar um padrão.

Existem diversas formas de combinar classificadores, algumas delas serão abordadas logo abaixo.


\subsection{Regra da Soma}
\label{subsubsec:regradasoma}

A regra da soma faz a combinação de classificadores somando as probabilidades a posteriori de cada classificador. Dado um padrão $x$, ele será classificado como define e equação \ref{eq:regradasoma}.

\begin{equation}
\label{eq:regradasoma}
(1 - L) p(\omega_j) \sum_{i = 1}^{L} p(\omega_j | \mathbf{x_i}) > (1 - L) p(\omega_k) \sum_{i = 1}^{L} p(\omega_k | \mathbf{x_i}),
\end{equation}
\[k = 1, \ldots, C;     k \ne j \] Sendo $C$ o número de classes e $L$ a quantidade de classificadores.

Para probabilidades a priori iguais, o padrão $x$ será classificado como sendo da classe que obtiver maior soma de probabilidades a posteriori.

Esta regra é adequada para classificadores que possuem o mesmo sensor, pois o mesmo não é muito sensível a erros na estimação de densidade. Para se obter melhores resultados, pode-se atribuir diferentes pesos para as probabilidades a posteriori de cada classificador.

Além da regra da soma, existem outras possíveis regras abordadas brevemente nas subseções abaixo.

\subsection{Regra do Produto}
\label{subsubsec:regradoproduto}

Uma forma de fazer a combinação de classificadores é utilizando a regra do produto. Esta regra, como o próprio nome sugere, nada mais é do que fazer a multiplicação das probabilidades a \textit{posteriori} de cada classificador.

Dado um padrão $x$, ele será classificado como sendo da classe que obtiver maior valor da multiplicação entre as probabilidades a posteriori de um classificador.

Esta técnica é adequada quando as entradas dos classificadores são providas por sensores diferentes.

\subsection{Regra do Máximo}
\label{subsubsec:regradomaximo}

A regra do máximo é uma aproximação da regra da soma. Nesta aproximação, a probabilidade de uma determinada classe é a maior probabilidade a posteriori dentre as saídas dos classificadores para aquela classe.

\subsection{Regra do Mínimo}
\label{subsubsec:regradominimo}

A regra do mínimo é uma aproximação da regra do produto. Nesta aproximação, a probabilidade de uma determinada classe é a menor das probabilidades a posteriori dentre as saídas dos classificadores para aquela classe.


\subsection{Regra da Mediana}
\label{subsubsec:regradamediana}

Esta regra é um caso particular onde é obtida as medianas das probabilidades a posteriori de cada classe. A classe que obtiver maior mediana é utilizada para classificar um padrão $x$ de entrada.




