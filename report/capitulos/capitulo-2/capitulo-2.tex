% !TEX encoding = ISO-8859-1
\chapter{Técnicas Avaliadas}
\label{ch:ch2}

\section{Contexto e Histórico}
\label{sec:contexto}

Esta sessão abordará sistemas de reconhecimento de padrões e classificadores com aprendizagem baseadas em instâncias.

\subsection{Sistemas de Reconhecimento de Padrões}
\label{subsec:sistemas}

No final dos anos 50, surgiram os primeiros trabalhos de aprendizagem de máquina. De uma forma geral, elas consistiam em dar ao computador a habilidade de reconhecer formas. A partir daí, surgiram diversos problemas onde a aprendizagem de máquina atuava. 

Existem três problemas gerais que a aprendizagem de máquina tenta resolver. Um deles é o problema do agrupamento, que consiste em agrupar dados de acordo com suas características, de forma que seja possível extrair informação útil destes agrupamentos. Um outro problema é a discriminação, que basicamente é achar uma forma de reconhecer um conceito, dado um conjunto de conceitos exemplos. O terceiro e último problema é o da generalização, que é o problema de como reduzir uma regra de classificação, tornando-a mais abrangente e menos custosa.

Reconhecimento de padrões ataca principalmente o problema da discriminação, tendo por objetivo classificar padrões, discriminando-os entre duas ou mais classes. A classificação pode ser feita com padrões pertencentes a qualquer domínio, como reconhecimento de digitais, gestos, escrita, fala, entre outros.

\section{Classificadores}
\label{sec:classificadores}

Todo sistema de reconhecimento de padrões utiliza um classificador para discriminar os padrões de teste. A eficiência de um classificador é medida pela taxa de acerto média, pela variância, e pelo seu custo computacional. Um classificador de aprendizagem baseada em instâncias muito utilizado é o \textit{K-Nearest Neighbor}, KNN [\cite{knnrule:1969}]. 

\subsection{K-Nearest Neighbor}
\label{subsec:knn}

O KNN é muito usado por ser um método de aprendizagem supervisionado simples, e por possuir uma taxa de acerto relativamente alta. O conceito básico consiste em: Dado um padrão $x$ a ser classificado e um conjunto de padrões conhecidos $T$, obter as classes dos $K$ elementos de $T$ mais próximos de $x$. A classe que obtiver maior ocorrência, ou peso, será a classe de $x$. Pode-se dizer que o KNN utiliza uma abordagem \textit{"Dize-me com quem andas, e direi quem és."}. O algoritmo esta descrito em Algorithm \ref{alg:knn}.

\begin{algorithm}[H]
\caption{KNN}
\label{alg:knn}
\begin{algorithmic}[1]
\REQUIRE {$K$: um número}
\REQUIRE {$T$: conjunto de treinamento}
\REQUIRE {$x$: elemento para ser classificado}
\REQUIRE {$L$: uma lista}
\FORALL {$t_i$ $\in$ $T$}
\STATE  $d_i$ = $distance(t_i, x)$
\STATE  adicione $(d_i, Classe(t_i))$ em $L$
\ENDFOR
\STATE $Ordene(L)$ de acordo com as distâncias
\STATE obtenha os $K$ primeiros elementos de $L$
\RETURN a classe de maior ocorrência, ou peso, entre os $K$
\end{algorithmic}
\end{algorithm}


Pode-se dizer que o $K-Nearest Neighbor$ atua considerando a densidade das classes na região onde o padrão que se deseja classificar. A estimação de densidade é baseada na probabilidade a posteriori.

\subsection{Janela de Parzen}
\label{subsec:janeladeparzen}

Janela de Parzen é uma técnica de estimação de densidade que se baseia na interpolação de dados. Esta estimativa é feita assumindo-se uma função $K(x)$ que determina a janela centrada em $x$ com largura $h$, esta função é chamada de função de kernel.

De maneira simples, para estimar a densidade fixa-se uma região $R$ e um volume fixo $V$, e $k$ correspondente é determinado a partir dos dados de aprendizagem. Assumindo que a região $R$ é um hipercubo de $d$ dimensões, o seu volume é dado por $h^d$.

\begin{equation}
\label{eq:parzen1}
p(x) = \frac{\frac{k}{n}}{V}
\end{equation}

Para se obter a função de densidade de probabilidade de uma amostra $x$ utilizando-se a Janela de Parzen, simplismente centra-se a região $R$ em $x$ e conta-se o número de instâncias dentro desta região (valor de $k$) na equação \ref{eq:parzen1}.

Determinar o $h$ ideal é uma tarefa minunciosa, pois, se o valor de $h$ for muito pequeno, haverá muita especialização e estará mais sujeita a erros ocasionados por ruídos, já se $h$ for muito grande, ocorrerá uma super-generalização. A escolha da função $K(x)$ apropriada também é um problema para esta técnica.

\subsection{Estimação da Máxima Verossimilhança}
\label{subsec:mle}

Estimativa por máxima verossimilhança, $Maximum-Likelihood$, é um método para estimar os parâmetros de um modelo estatístico. A partir de um conjunto de dados e um dado modelo estatístico (i.e. Normal), o $MLE$ estima valores adequados para os parâmetros do modelo.

De forma geral, dado um conjunto de instâncias de treinamento e um modelo estatístico, o $MLE$ estima os valores dos parâmetros do modelo estatístico de forma que a probabilidade dos dados observados sejam maximizados.

\subsection{Algoritmo Expectation-Maximization}
\label{subsec:em}
Expectation-Maximization é um algoritmo iterativo para se encontrar a máxima verossimilhança, os parâmetros de um modelo, onde o modelo é desconhecido, e depende de variáveis não obsersevadas. 

A iteração do $EM$ alterna entre realizar a Expectation $E$ e a Maximization $M$. A $E$ computa o valor esperado da log-verossimilhança avaliada, utilizando a estimativa atual para os parâmetros. Já a $M$ computa os parâmetros, maximizando a esperada log-verossimilhança encontrada em $E$.

A diferença do $EM$ para o $MLE$ é que o $EM$ é capaz de tratar distribuições multimodais, além de ser capaz de estimar os parâmetros mesmo com $missing values$.


\subsection{Combinação de Classificadores}

\section{Agrupamento}
\subsection{K-Means}



